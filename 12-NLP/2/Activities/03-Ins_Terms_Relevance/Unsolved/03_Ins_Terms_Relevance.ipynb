{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Instructor Do: Terms Relevance (Understanding TF-IDF)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Initial imports\nimport nltk\nfrom nltk.corpus import reuters\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n"},{"cell_type":"markdown","metadata":{},"source":["## Loading Text from the Reuters Dataset\n","\n","To demonstrate how TF-IDF works, we will use the _Reuters_ dataset that is bundled in NLTK."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Download/update the Reuters dataset\n\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Count the total number of documents in the collection\n\n"},{"cell_type":"markdown","metadata":{},"source":["## Getting Bag of Words from a Single Document\n","\n","We select a single document from the corpus to get it's \"Bag of Words\". The same can be done from multiple documents by pasing a list of documents (or documents ids on this example) to the `CountVectorizer()` object."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Select and print the original single document text\n\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Creating the CountVectorizer instance defining the stop words in English to be ignored\n\n\n# Getting the tokenization and occurrence counting\n\n\n# Retrieve unique words list\n\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# X raw data contains the occurrence of each term in the document. A unique ID is assigned to each term.\n\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Getting the bag of words as DataFrame\n\n"},{"cell_type":"markdown","metadata":{},"source":["## Calculating the TF-IDF from a Corpus"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Getting the corpus (first 1000 files from Reuters dataset)\n\n\n# Print sample documen\n\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Getting the TF-IDF\n\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Getting matrix info\n\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Retrieve words list from corpous\n\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Getting the TF-IDF weight of each word in corpus as DataFrame\n\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Highest 10 TF-IDF scores\n\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Lowest 10 TF-IDF scores\n"}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}